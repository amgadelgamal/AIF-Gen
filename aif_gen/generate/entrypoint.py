import argparse
import asyncio
import logging
import pathlib
import time

from aif_gen.api.response_mapper import ResponseMapper
from aif_gen.dataset import AlignmentDataset
from aif_gen.generate.service import process_prompts
from aif_gen.task import AlignmentTask, Domain
from aif_gen.util.logging import setup_basic_logging

parser = argparse.ArgumentParser(
    description='Generate an AlignmentDataset',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument(
    '--model_name', type=str, required=True, help='OpenAI compatible model alias.'
)
parser.add_argument(
    '--log_file_name',
    type=str,
    default=f'aif_generation.log',
    help='Name of the log file to write to within the output directory.',
)
parser.add_argument(
    '--output_path',
    type=str,
    default=f'data/{time.time()}',
    help='Path to directory where to save the dataset.',
)
parser.add_argument(
    '--save_frequency',
    type=int,
    default=500,
    help='Number of samples to batch write to disc.',
)
parser.add_argument(
    '--max_concurrency',
    type=int,
    default=256,
    help='Max number of concurrent API inference requests to the language model.',
)
parser.add_argument(
    '--num_samples',
    type=int,
    default=1000,
    help='Number of response pairs to generate for th egiven task prompt.',
)


async def main() -> None:
    args = parser.parse_args()
    async_semaphore = asyncio.Semaphore(args.max_concurrency)

    output_path = pathlib.Path(args.output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    log_file = output_path / args.log_file_name
    setup_basic_logging(log_file)

    task = get_mock_alignment_task()

    logging.info(f'Generating AIF Dataset for task: {task}')
    logging.info(f'Using Model: {args.model_name}')

    prompts = []
    prompts.append(generate_mock_prompt(task, args.num_samples))

    batch_index: int = 0
    dataset = AlignmentDataset(task, [])
    async for sample in process_prompts(
        prompts, model_name=args.model_name, async_semaphore=async_semaphore
    ):
        if sample is not None:
            dataset.append(sample)

        if (len(dataset) + 1) % args.save_frequency == 0:
            write_dataset_batch(dataset, output_path, batch_index)
            dataset = AlignmentDataset(task, [])  # Should be able to just clear it
            batch_index += 1

    output_file_path = output_path / f'output_{batch_index:03d}.json'
    dataset.to_json(output_file_path)


def write_dataset_batch(
    dataset: AlignmentDataset, output_path: pathlib.Path, batch_index: int
) -> None:
    output_file_path = output_path / f'output_{batch_index:03d}.json'
    logging.info(f'Writing batch with {len(dataset)} samples to {output_file_path}')
    dataset.to_json(output_file_path)
    logging.info(f'Wrote batch with {len(dataset)} samples to {output_file_path}')


def generate_mock_prompt(task: AlignmentTask, num_samples: int) -> str:
    logging.info(f'Generating response prompt for {num_samples} samples')

    # In theory, this would have been generated by the prompt mapper
    task_prompt = task.objective

    response_mapper = ResponseMapper()
    prompt = response_mapper.generate_prompt(task, task_prompt, num_samples=num_samples)
    logging.info(f'Using prompt:\n\n{prompt}')
    return prompt


def get_mock_alignment_task() -> AlignmentTask:
    logging.info('Generating mock alignment task')
    component_dict = {
        'Healthcare': {
            'seed_words': ['hospital', 'exercise', 'medicine'],
        },
        'Technology': {
            'seed_words': ['engineering', 'AI', 'internet'],
        },
    }
    domain = Domain.from_dict(component_dict)
    objective = 'Generate a news article headline.'
    preference = 'Make the headline polarizing'

    task = AlignmentTask(domain, objective, preference)
    return task


if __name__ == '__main__':
    asyncio.run(main())
